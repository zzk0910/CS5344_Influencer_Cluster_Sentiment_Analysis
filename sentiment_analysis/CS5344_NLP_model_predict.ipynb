{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23900,
     "status": "ok",
     "timestamp": 1667184174918,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "pg4zIzDkaztl",
    "outputId": "9adf52bb-3b62-4f8f-bc59-141878454eb0"
   },
   "outputs": [],
   "source": [
    "# If using Google colab, mount the Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2393,
     "status": "ok",
     "timestamp": 1667184177308,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "L92PR2IBcosb",
    "outputId": "6aba2d8e-4b26-4b02-8667-281ed294f25e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import copy\n",
    "from typing import Union\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 15:15:16.444133: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1667184177800,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "90Clj-xwetEN"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2743,
     "status": "ok",
     "timestamp": 1667184180541,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "q7KXGxv2dCBY",
    "outputId": "f92facff-9fe3-4b09-d21f-724d004fde22"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5n/ccq0r8d93p5d9t3_5c2p0j0h0000gn/T/ipykernel_44971/1703566803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras import Sequential, Model, constraints\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional, Masking, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ-bFd_d_X1Z"
   },
   "source": [
    "### **Load necessary files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zzkzz/Documents/CS5344/Project/CS5344_Influencer_Cluster_Sentiment_Analysis/sentiment_analysis'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1667184181488,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "_ZOJMbCWcnoK"
   },
   "outputs": [],
   "source": [
    "# Load necessary files\n",
    "\n",
    "with open(os.path.join(root_path, 'data/english_contractions.json'), 'r') as f1:\n",
    "    eng_contractions = json.load(f1)\n",
    "f1.close()\n",
    "\n",
    "with open(os.path.join(root_path, 'data/vocab_to_idx.json'), 'r') as f2:\n",
    "    word_to_idx_dict = json.load(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azOcJzjWVpER"
   },
   "source": [
    "### **Define key parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667184181488,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "CdC91vJsmFAj"
   },
   "outputs": [],
   "source": [
    "seqLen = 15\n",
    "minLen = 8\n",
    "neutral_threshold = 0.8\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Recording the relationship between idx and class\n",
    "class_idx_dict = {'neutral':-1, 'happiness':0, 'fun':1, 'sadness':2, 'hate':3} # Since `neutral` label is not directly included in our prediction, we mark it as `-1`\n",
    "idx_class_dict = {idx:class_label for class_label, idx in class_idx_dict.items()}\n",
    "\n",
    "num_classes = len(class_idx_dict) - 1\n",
    "vocab_size = len(word_to_idx_dict) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667184181489,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "oDaJzVn0gpIi"
   },
   "outputs": [],
   "source": [
    "# Sample sentence\n",
    "# sentence = 'I feel bad today. I think I should go to see a doctor. @father'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWjrwWx0_UqA"
   },
   "source": [
    "### **Define necessary preprocessing functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667184181489,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "RHdlpLZ9a7V1"
   },
   "outputs": [],
   "source": [
    "def normalize_contractions(sentence, eng_contractions_dict):\n",
    "    return _normalize_contractions_text(sentence, eng_contractions_dict)\n",
    "\n",
    "def _normalize_contractions_text(text, contractions):\n",
    "    \"\"\"\n",
    "    This function normalizes english contractions (all input sentences in lower case).\n",
    "    \"\"\"\n",
    "    new_token_list = []\n",
    "    token_list = text.split()\n",
    "    for word_pos in range(len(token_list)):\n",
    "        word = token_list[word_pos]\n",
    "        if word in contractions:\n",
    "            replacement = contractions[word]\n",
    "\n",
    "            first_rep = replacement.strip().split('/')[0]\n",
    "            replacement_tokens = first_rep.strip().split()\n",
    "            for w in replacement_tokens:\n",
    "                new_token_list.append(w)\n",
    "        else:\n",
    "            new_token_list.append(word)\n",
    "    sentence = \" \".join(new_token_list).strip(\" \")\n",
    "    return sentence\n",
    "\n",
    "def simplify_punctuation_and_whitespace(sentence):\n",
    "\n",
    "    # print(\"Normalizing whitespaces and punctuation\")\n",
    "    sent = _replace_urls(sentence)\n",
    "    sent = _simplify_punctuation(sent)\n",
    "    simplified_sent = _normalize_whitespace(sent)\n",
    "      \n",
    "    return simplified_sent\n",
    "\n",
    "def _replace_urls(text):\n",
    "    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    text = re.sub(url_regex, \"<URL>\", text)\n",
    "    return text\n",
    "\n",
    "def _simplify_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function simplifies doubled or more complex punctuation. The exception is '...'.\n",
    "    \"\"\"\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n",
    "    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n",
    "    return corrected\n",
    "\n",
    "def _normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    This function normalizes whitespaces, removing duplicates.\n",
    "    \"\"\"\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
    "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
    "    return corrected.strip(\" \")\n",
    "\n",
    "def reduce_exaggerations(text):\n",
    "    \"\"\"\n",
    "    Auxiliary function to help with exxagerated words.\n",
    "    Examples:\n",
    "        woooooords -> words\n",
    "        yaaaaaaaaaaaaaaay -> yay\n",
    "    \"\"\"\n",
    "    correction = str(text)\n",
    "    #TODO work on complexity reduction.\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', correction)\n",
    "\n",
    "def sentence_tokenizer(tk, sentence, word_to_idx_dict):\n",
    "    words_list = tk.tokenize(sentence)\n",
    "    tokenized_words_list = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(words_list):\n",
    "        if words_list[i].startswith('http') or words_list[i].startswith('www.'):\n",
    "            i += 1\n",
    "        elif words_list[i].endswith('.com'):\n",
    "            i += 1\n",
    "        # elif words_list[i-1] in string.punctuation and words_list[i] in string.punctuation:\n",
    "        #     i += 1\n",
    "        elif words_list[i] in string.punctuation:\n",
    "            i += 1\n",
    "        elif (len(words_list[i]) > 1) and (not (ord('a') <= ord(words_list[i][0]) <= ord('z'))) and (not words_list[i].startswith('<')):\n",
    "            i += 1\n",
    "        elif words_list[i].startswith('@'):\n",
    "            tokenized_words_list.append('<person>')\n",
    "            i += 1\n",
    "        else:\n",
    "            tokenized_words_list.append(words_list[i])\n",
    "            i += 1\n",
    "\n",
    "    for j, w in enumerate(tokenized_words_list):\n",
    "        if w not in word_to_idx_dict:\n",
    "            tokenized_words_list[j] = 'unk'\n",
    "\n",
    "    return tokenized_words_list\n",
    "\n",
    "\n",
    "# Main\n",
    "\n",
    "def preprocess_user_sentence(sentence, seqLen, eng_contractions, \n",
    "                             word_to_idx_dict, \n",
    "                             tokenizer):\n",
    "    sentence = sentence.lower()\n",
    "    s = normalize_contractions(sentence, eng_contractions)\n",
    "    s = simplify_punctuation_and_whitespace(s)\n",
    "    s = reduce_exaggerations(s)\n",
    "    tokens = sentence_tokenizer(tokenizer, sentence, word_to_idx_dict)\n",
    "    tokens_in_numbers = [word_to_idx_dict[w] for w in tokens]\n",
    "    \n",
    "    return tokens_in_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667184181489,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "JiXWMJzv_5SD"
   },
   "outputs": [],
   "source": [
    "def tweet_preprocess(inputs: Union[pd.DataFrame, pd.Series], minLen=8, tokenizer=TweetTokenizer(), sentence_tokenize=False):\n",
    "    \"\"\"\n",
    "    Function to preprocess tweets\n",
    "    inputs: A dataframe in the format of |id|tweet1|tweet2|...|tweetn|, |id|tweet|, or |tweet|\n",
    "    outputs: A dataframe in the format of |id|tweet_index|tweet|tokens|most_common_sentiment|\n",
    "    sentence_tokenize: Determine if need to split text into sentences\n",
    "    \"\"\"\n",
    "\n",
    "    data = inputs.copy(deep=True)\n",
    "\n",
    "    if data.shape[1]>2: # If there are various tweety in one row, transform them into the format of |id|tweet|\n",
    "        data.columns = ['id'] + ['tweets_'+str(i) for i in range(1, data.shape[1])]\n",
    "        data = data.set_index(['id'])\n",
    "        data = data.stack()\n",
    "        data = data.reset_index()\n",
    "        data.columns = ['id', 'tweet_index', 'tweet']\n",
    "        # data = data.drop(['tweet_index'], axis=1)\n",
    "        # data.dropna()\n",
    "\n",
    "    elif data.shape[1]==2: # Data is in the format of |id|tweet|\n",
    "        data.columns = ['id', 'tweet']\n",
    "\n",
    "    else: # Data is in the format of |tweet|\n",
    "        data.columns = ['tweet']\n",
    "    \n",
    "    if sentence_tokenize: # Determine whether to split text into sentences\n",
    "        data = sentences_tokenizer(data)\n",
    "    \n",
    "    # Standize some characters\n",
    "    data['tweet'] = data['tweet'].map(lambda tweet: tweet.replace('¡¯', '\\'').replace('¡', '...'))\n",
    "\n",
    "    # Tokenize text\n",
    "    data['tokens'] = data['tweet'].map(lambda tweet: preprocess_user_sentence(tweet, seqLen, eng_contractions, word_to_idx_dict, tokenizer)[0])\n",
    "    data = data[data['tokens'].map(lambda x: len(x) > minLen)]\n",
    "\n",
    "    padded_sentence = pad_sequences(data['tokens'], maxlen=seqLen, padding='post', truncating='pre')\n",
    "    data['tokens'] = [list(doc) for doc in padded_sentence]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def tweet_sentiment_predict(tweet_df, threshold, trained_model, save_file=False):\n",
    "    \"\"\"\n",
    "    Function to predict tweet sentiment for different users\n",
    "    threshold: When the highest score is lower than threshold, predicted label is set as `neutral`\n",
    "    outputs: A dataframe in the format of |id|neutral|happiness|...|most_common_sentiment|; A dictionary containing ids and corresponding sentiments\n",
    "    \"\"\"\n",
    "    # Predict\n",
    "    type_scores = trained_model.predict(np.asarray(tweet_df['tokens'].values.tolist()))\n",
    "\n",
    "    # Decode sentiment\n",
    "    highest_scores = [np.max(score, axis=-1) for score in type_scores]\n",
    "    idx_highest_scores = [np.argmax(score, axis=-1) for score in type_scores]\n",
    "    labels = [idx_class_dict[idx] if score>=threshold else idx_class_dict[-1] for idx, score in zip(idx_highest_scores, highest_scores)]\n",
    "    tweet_df[\"tag\"] = pd.DataFrame(labels)\n",
    "\n",
    "    # Drop useless column\n",
    "    # if 'tweet_index' in tweet_df.columns:\n",
    "    #     tweet_df = tweet_df.drop(['token'], axis=1)\n",
    "    # if 'tweet' in tweet_df.columns:\n",
    "    #     tweet_df = tweet_df.drop(['sentence'], axis=1)\n",
    "\n",
    "    # Count # of each type\n",
    "    tweet_df = tweet_df.groupby(['id', 'tag'])['tag'].count()\n",
    "    tweet_df = tweet_df.unstack()\n",
    "    tweet_df = tweet_df.fillna(0)\n",
    "    tweet_df = tweet_df.reset_index()\n",
    "    tweet_df.iloc[:,1:] = tweet_df.iloc[:,1:].astype('int64')\n",
    "    tweet_df.iloc[:,1:] = tweet_df.iloc[:,1:].div(tweet_df.iloc[:,1:].sum(axis=1), axis=0).round(4)\n",
    "    \n",
    "    # Find the most common type\n",
    "    sentiment_df = tweet_df.iloc[:,1:]\n",
    "    tweet_df['most_common_sentiment'] = sentiment_df.idxmax(axis=1)\n",
    "\n",
    "    sentiment_dict = {}\n",
    "    ids, sentiments = tweet_df['id'].values.tolist(), tweet_df['most_common_sentiment'].values.tolist()\n",
    "    for i in range(len(ids)):\n",
    "        sentiment_dict[ids[i]] = sentiments[i]\n",
    "\n",
    "    if save_file:\n",
    "        tweet_df.to_csv(os.path.join(root_path, 'data/sentiment.txt'), sep=',', index=False)\n",
    "\n",
    "    return tweet_df, sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667184181490,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "ke_JsNvbJUac"
   },
   "outputs": [],
   "source": [
    "def postprocessing(id_df, sentiment_df, id_to_sentiment_dict):\n",
    "    all_ids = pd.unique(id_df['id'])\n",
    "    has_sentiment_label_ids = set(pd.unique(sentiment_df['id']))\n",
    "\n",
    "    # Add labels for those ids without valid sentences\n",
    "    for id in all_ids:\n",
    "        if id not in has_sentiment_label_ids:\n",
    "            id_to_sentiment_dict[id] = 'neutral'\n",
    "\n",
    "    # Combine id_df and sentiment_df to get final result\n",
    "    sentiment_predicted_results = pd.merge(id_df, sentiment_df, on=['id']).sort_values(['center', 'id'])\n",
    "    \n",
    "    return sentiment_predicted_results, id_to_sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667184181490,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "PUYvNrmhAL83"
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentences_tokenizer(inputs: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Auxiliary function to split text into sentences.\n",
    "    inputs: A dataframe in the format of |user_id|tweet|\n",
    "    outputs: A dataframe in the format of |user_id|sentences|\n",
    "    Examples:\n",
    "        I feel bad today. I think I should go to see a doctor. -> I feel bad today.\n",
    "                                            I think I should go to see a doctor.\n",
    "    \"\"\"\n",
    "    outputs = inputs.copy(deep=True)\n",
    "    outputs[\"sentences\"] = outputs[\"tweet\"].map(lambda tweet: sent_tokenize(tweet))\n",
    "    outputs = outputs.drop([\"tweet\"], axis=1)\n",
    "    outputs = outputs.explode(\"sentences\")\n",
    "    outputs.columns = ['id', \"sentence\"]\n",
    "    outputs = outputs.dropna()\n",
    "\n",
    "    outputs = outputs.reset_index()\n",
    "    outputs = outputs.drop([\"index\"], axis=1)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1247WWplxNP1"
   },
   "source": [
    "### **Option A: Load the whole model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6919,
     "status": "ok",
     "timestamp": 1667184188404,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "55h3AsZlecYy"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = os.path.join(root_path, 'lstm_model_v1')\n",
    "# print(model_path)\n",
    "trained_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667184188404,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "8z9_Ybw6jJMJ"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "# tk_test = TweetTokenizer()\n",
    "# tokens = preprocess_user_sentence(sentence, seqLen, eng_contractions, word_to_idx_dict, tk_test)\n",
    "# tokens_in_numbers = get_tokens_in_numbers(tokens, seqLen)\n",
    "# type_scores = trained_model.predict(tokens_in_numbers).tolist()\n",
    "\n",
    "# # 每个sample sentence会得到四个score，分别对应'happiness', 'fun', 'sadness', 'hate'\n",
    "# 选择取最高的score，如果最高score大于某个阈值（例如0.7），就判断为该类，如果最高的score达不到阈值，判断为没有特殊感情的neutral类\n",
    "# print(type_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGD9Soh_xfHi"
   },
   "source": [
    "### **Option B: Load model from a checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667184188404,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "U5M6BdIJlm-w"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(seqLen, num_classes, vocab_size):\n",
    "    input_tensor = Input(shape=(seqLen,), dtype='int32')\n",
    "    mask = Masking(mask_value=0, input_shape=(seqLen, 50))(input_tensor)\n",
    "    x = Embedding(vocab_size, 50, input_length=seqLen, trainable=False)(mask)\n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = LSTM(64, return_sequences=False)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    model.compile(optimizer=Adam(learning_rate=3e-4),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1714,
     "status": "ok",
     "timestamp": 1667184190108,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "7gjIHPSVlpmJ",
    "outputId": "26257ae9-129a-4f04-eb83-66d4a8371916"
   },
   "outputs": [],
   "source": [
    "# Load model from checkpoint dir\n",
    "checkpoint_dir = os.path.join(root_path, 'training_1')\n",
    "latest_cp = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# Create a new model instance\n",
    "trained_model = create_lstm_model(seqLen, num_classes, vocab_size)\n",
    "\n",
    "# Load the previously saved weights\n",
    "trained_model.load_weights(latest_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1553,
     "status": "ok",
     "timestamp": 1667184191659,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "-He8q0GnluXI",
    "outputId": "8f9d1e6a-1c86-4e37-f154-5815b27c87cf"
   },
   "outputs": [],
   "source": [
    "# # Sample sentence\n",
    "# sentence = 'I feel bad today. I think I should go to see a doctor. @father'\n",
    "# # Test\n",
    "# tk_test = TweetTokenizer()\n",
    "# tokens = preprocess_user_sentence(sentence, seqLen, eng_contractions, word_to_idx_dict, tk_test)\n",
    "# # tokens_in_numbers = get_tokens_in_numbers(tokens, seqLen)\n",
    "# # type_scores = trained_model.predict(tokens_in_numbers).tolist()\n",
    "# type_scores = trained_model.predict(tokens).tolist()\n",
    "\n",
    "# # 每个sample sentence会得到四个score，分别对应'happiness', 'fun', 'sadness', 'hate'\n",
    "# # 选择取最高的score，如果最高score大于某个阈值（例如0.7），就判断为该类，如果最高的score达不到阈值，判断为没有特殊感情的neutral类\n",
    "# print(type_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUDoX7ebxy02"
   },
   "source": [
    "### **Make predictions based on user tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2035,
     "status": "ok",
     "timestamp": 1667184193690,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "VeZC0owgyORt"
   },
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(os.path.join(root_path, 'data/neighbor.csv'), \n",
    "                       encoding = 'unicode_escape',\n",
    "                       dtype={'id': str},\n",
    "                       na_filter=False)\n",
    "input_df.drop_duplicates(subset=['id'], keep='first', inplace=True, ignore_index=True)\n",
    "id_df = input_df[['id', 'center']]\n",
    "inputs = input_df.drop(['center'], axis=1)\n",
    "user_tweet_df = tweet_preprocess(inputs=inputs, minLen=minLen, tokenizer=tokenizer,sentence_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667184193690,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "VVwQdAqlN6oI",
    "outputId": "97dcadad-df77-4f63-90a6-e5dbf9a90660"
   },
   "outputs": [],
   "source": [
    "user_tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4116,
     "status": "ok",
     "timestamp": 1667184197795,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "rM8I2Wsk74u8",
    "outputId": "87f758a8-657f-4a47-bbd6-d5982ccba92d"
   },
   "outputs": [],
   "source": [
    "sentiment_df, partial_id_to_sentiment_dict = tweet_sentiment_predict(user_tweet_df, threshold=neutral_threshold, trained_model=trained_model, save_file=True)\n",
    "sentiment_predicted_results, id_to_sentiment_dict = postprocessing(id_df, sentiment_df, partial_id_to_sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667184197795,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "gt0BR-2fFhwL",
    "outputId": "9f26df88-4b49-4e44-90c6-03b220910f23"
   },
   "outputs": [],
   "source": [
    "print(id_to_sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667184197796,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "N2CmCx2lHok5",
    "outputId": "aa74d108-1846-4a8a-97e9-f9cf916021fc"
   },
   "outputs": [],
   "source": [
    "sentiment_predicted_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667184197796,
     "user": {
      "displayName": "He Wenbin",
      "userId": "03124039579078048321"
     },
     "user_tz": -480
    },
    "id": "9PL3yrvBFHib"
   },
   "outputs": [],
   "source": [
    "# Save id -> sentiment labels\n",
    "with open(os.path.join(root_path, 'data/neightbor_sentiments.csv'), 'w') as f:\n",
    "    s = json.dumps(id_to_sentiment_dict)\n",
    "    f.write(s)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
