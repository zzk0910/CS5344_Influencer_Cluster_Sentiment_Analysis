{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pg4zIzDkaztl","outputId":"9adf52bb-3b62-4f8f-bc59-141878454eb0","executionInfo":{"status":"ok","timestamp":1667184174918,"user_tz":-480,"elapsed":23900,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import re\n","import string\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import json\n","import copy\n","from typing import Union\n","from nltk.tokenize import TweetTokenizer\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L92PR2IBcosb","outputId":"6aba2d8e-4b26-4b02-8667-281ed294f25e","executionInfo":{"status":"ok","timestamp":1667184177308,"user_tz":-480,"elapsed":2393,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import gensim\n","from gensim.models.keyedvectors import KeyedVectors"],"metadata":{"id":"90Clj-xwetEN","executionInfo":{"status":"ok","timestamp":1667184177800,"user_tz":-480,"elapsed":495,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","print(tf.__version__)\n","\n","from tensorflow.keras import Sequential, Model, constraints\n","from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional, Masking, Layer\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7KXGxv2dCBY","outputId":"f92facff-9fe3-4b09-d21f-724d004fde22","executionInfo":{"status":"ok","timestamp":1667184180541,"user_tz":-480,"elapsed":2743,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.2\n"]}]},{"cell_type":"markdown","source":["### **Load necessary files**"],"metadata":{"id":"jZ-bFd_d_X1Z"}},{"cell_type":"code","source":["# Load necessary files\n","with open('/content/drive/MyDrive/Colab Notebooks/CS5344/data/english_contractions.json', 'r') as f1:\n","    eng_contractions = json.load(f1)\n","f1.close()\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/CS5344/data/vocab_to_idx.json', 'r') as f2:\n","    word_to_idx_dict = json.load(f2)"],"metadata":{"id":"_ZOJMbCWcnoK","executionInfo":{"status":"ok","timestamp":1667184181488,"user_tz":-480,"elapsed":956,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### **Define key parameters**"],"metadata":{"id":"azOcJzjWVpER"}},{"cell_type":"code","source":["seqLen = 15\n","minLen = 8\n","neutral_threshold = 0.8\n","tokenizer = TweetTokenizer()\n","\n","# Recording the relationship between idx and class\n","class_idx_dict = {'neutral':-1, 'happiness':0, 'fun':1, 'sadness':2, 'hate':3} # Since `neutral` label is not directly included in our prediction, we mark it as `-1`\n","idx_class_dict = {idx:class_label for class_label, idx in class_idx_dict.items()}\n","\n","num_classes = len(class_idx_dict)-1\n","vocab_size = len(word_to_idx_dict) + 1"],"metadata":{"id":"CdC91vJsmFAj","executionInfo":{"status":"ok","timestamp":1667184181488,"user_tz":-480,"elapsed":7,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Sample sentence\n","# sentence = 'I feel bad today. I think I should go to see a doctor. @father'"],"metadata":{"id":"oDaJzVn0gpIi","executionInfo":{"status":"ok","timestamp":1667184181489,"user_tz":-480,"elapsed":7,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### **Define necessary preprocessing functions**"],"metadata":{"id":"fWjrwWx0_UqA"}},{"cell_type":"code","source":["def normalize_contractions(sentence, eng_contractions_dict):\n","    return _normalize_contractions_text(sentence, eng_contractions_dict)\n","\n","def _normalize_contractions_text(text, contractions):\n","    \"\"\"\n","    This function normalizes english contractions (all input sentences in lower case).\n","    \"\"\"\n","    new_token_list = []\n","    token_list = text.split()\n","    for word_pos in range(len(token_list)):\n","        word = token_list[word_pos]\n","        if word in contractions:\n","            replacement = contractions[word]\n","\n","            first_rep = replacement.strip().split('/')[0]\n","            replacement_tokens = first_rep.strip().split()\n","            for w in replacement_tokens:\n","                new_token_list.append(w)\n","        else:\n","            new_token_list.append(word)\n","    sentence = \" \".join(new_token_list).strip(\" \")\n","    return sentence\n","\n","def simplify_punctuation_and_whitespace(sentence):\n","\n","    # print(\"Normalizing whitespaces and punctuation\")\n","    sent = _replace_urls(sentence)\n","    sent = _simplify_punctuation(sent)\n","    simplified_sent = _normalize_whitespace(sent)\n","      \n","    return simplified_sent\n","\n","def _replace_urls(text):\n","    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n","    text = re.sub(url_regex, \"<URL>\", text)\n","    return text\n","\n","def _simplify_punctuation(text):\n","    \"\"\"\n","    This function simplifies doubled or more complex punctuation. The exception is '...'.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n","    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n","    return corrected\n","\n","def _normalize_whitespace(text):\n","    \"\"\"\n","    This function normalizes whitespaces, removing duplicates.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n","    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n","    return corrected.strip(\" \")\n","\n","def reduce_exaggerations(text):\n","    \"\"\"\n","    Auxiliary function to help with exxagerated words.\n","    Examples:\n","        woooooords -> words\n","        yaaaaaaaaaaaaaaay -> yay\n","    \"\"\"\n","    correction = str(text)\n","    #TODO work on complexity reduction.\n","    return re.sub(r'(.)\\1+', r'\\1\\1', correction)\n","\n","def sentence_tokenizer(tk, sentence, word_to_idx_dict):\n","    words_list = tk.tokenize(sentence)\n","    tokenized_words_list = []\n","    i = 0\n","\n","    while i < len(words_list):\n","        if words_list[i].startswith('http') or words_list[i].startswith('www.'):\n","            i += 1\n","        elif words_list[i].endswith('.com'):\n","            i += 1\n","        # elif words_list[i-1] in string.punctuation and words_list[i] in string.punctuation:\n","        #     i += 1\n","        elif words_list[i] in string.punctuation:\n","            i += 1\n","        elif (len(words_list[i]) > 1) and (not (ord('a') <= ord(words_list[i][0]) <= ord('z'))) and (not words_list[i].startswith('<')):\n","            i += 1\n","        elif words_list[i].startswith('@'):\n","            tokenized_words_list.append('<person>')\n","            i += 1\n","        else:\n","            tokenized_words_list.append(words_list[i])\n","            i += 1\n","\n","    for j, w in enumerate(tokenized_words_list):\n","        if w not in word_to_idx_dict:\n","            tokenized_words_list[j] = 'unk'\n","\n","    return tokenized_words_list\n","\n","\n","# Main\n","# def preprocess_user_sentence(sentence, eng_contractions,\n","#                              word_to_idx_dict, \n","#                              tokenizer):\n","#     sentence = sentence.lower()\n","#     s = normalize_contractions(sentence, eng_contractions)\n","#     s = simplify_punctuation_and_whitespace(s)\n","#     s = reduce_exaggerations(s)\n","#     tokens = sentence_tokenizer(tokenizer, sentence, word_to_idx_dict)\n","#     tokens_in_numbers = [word_to_idx_dict[w] for w in tokens]\n","\n","#     return tokens_in_numbers\n","\n","def preprocess_user_sentence(sentence, seqLen, eng_contractions, \n","                             word_to_idx_dict, \n","                             tokenizer):\n","    sentence = sentence.lower()\n","    s = normalize_contractions(sentence, eng_contractions)\n","    s = simplify_punctuation_and_whitespace(s)\n","    s = reduce_exaggerations(s)\n","    tokens = sentence_tokenizer(tokenizer, sentence, word_to_idx_dict)\n","    tokens_in_numbers = [word_to_idx_dict[w] for w in tokens]\n","    tokens_in_numbers = pad_sequences([tokens_in_numbers], maxlen=seqLen, padding='post')\n","    \n","    return tokens_in_numbers"],"metadata":{"id":"RHdlpLZ9a7V1","executionInfo":{"status":"ok","timestamp":1667184181489,"user_tz":-480,"elapsed":7,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def tweet_preprocess(inputs: Union[pd.DataFrame, pd.Series], minLen=8, tokenizer=TweetTokenizer(), sentence_tokenize=False):\n","    \"\"\"\n","    Function to preprocess tweets\n","    inputs: A dataframe in the format of |id|tweet1|tweet2|...|tweetn|, |id|tweet|, or |tweet|\n","    outputs: A dataframe in the format of |id|tweet_index|tweet|tokens|most_common_sentiment|\n","    sentence_tokenize: Determine if need to split text into sentences\n","    \"\"\"\n","\n","    data = inputs.copy(deep=True)\n","\n","    if data.shape[1]>2: # If there are various tweety in one row, transform them into the format of |id|tweet|\n","        data.columns = ['id'] + ['tweets_'+str(i) for i in range(1, data.shape[1])]\n","        data = data.set_index(['id'])\n","        data = data.stack()\n","        data = data.reset_index()\n","        data.columns = ['id', 'tweet_index', 'tweet']\n","        # data = data.drop(['tweet_index'], axis=1)\n","        # data.dropna()\n","\n","    elif data.shape[1]==2: # Data is in the format of |id|tweet|\n","        data.columns = ['id', 'tweet']\n","\n","    else: # Data is in the format of |tweet|\n","        data.columns = ['tweet']\n","    \n","    if sentence_tokenize: # Determine whether to split text into sentences\n","        data = sentences_tokenizer(data)\n","    \n","    # Standize some characters\n","    data['tweet'] = data['tweet'].map(lambda tweet: tweet.replace('¡¯', '\\'').replace('¡', '...'))\n","\n","    # Tokenize text\n","    data['tokens'] = data['tweet'].map(lambda tweet: preprocess_user_sentence(tweet, seqLen, eng_contractions, word_to_idx_dict, tokenizer)[0])\n","    data = data[data['tokens'].map(lambda x: len(x) > minLen)]\n","\n","    padded_sentence = pad_sequences(data['tokens'], maxlen=seqLen, padding='post', truncating='pre')\n","    data['tokens'] = [list(doc) for doc in padded_sentence]\n","\n","    return data\n","\n","\n","def tweet_sentiment_predict(tweet_df, threshold, trained_model, save_file=False):\n","    \"\"\"\n","    Function to predict tweet sentiment for different users\n","    threshold: When the highest score is lower than threshold, predicted label is set as `neutral`\n","    outputs: A dataframe in the format of |id|neutral|happiness|...|most_common_sentiment|; A dictionary containing ids and corresponding sentiments\n","    \"\"\"\n","    # Predict\n","    type_scores = trained_model.predict(np.asarray(tweet_df['tokens'].values.tolist()))\n","\n","    # Decode sentiment\n","    highest_scores = [np.max(score, axis=-1) for score in type_scores]\n","    idx_highest_scores = [np.argmax(score, axis=-1) for score in type_scores]\n","    labels = [idx_class_dict[idx] if score>=threshold else idx_class_dict[-1] for idx, score in zip(idx_highest_scores, highest_scores)]\n","    tweet_df[\"tag\"] = pd.DataFrame(labels)\n","\n","    # Drop useless column\n","    # if 'tweet_index' in tweet_df.columns:\n","    #     tweet_df = tweet_df.drop(['token'], axis=1)\n","    # if 'tweet' in tweet_df.columns:\n","    #     tweet_df = tweet_df.drop(['sentence'], axis=1)\n","\n","    # Count # of each type\n","    tweet_df = tweet_df.groupby(['id', 'tag'])['tag'].count()\n","    tweet_df = tweet_df.unstack()\n","    tweet_df = tweet_df.fillna(0)\n","    tweet_df = tweet_df.reset_index()\n","    tweet_df.iloc[:,1:] = tweet_df.iloc[:,1:].astype('int64')\n","    tweet_df.iloc[:,1:] = tweet_df.iloc[:,1:].div(tweet_df.iloc[:,1:].sum(axis=1), axis=0).round(4)\n","    \n","    # Find the most common type\n","    sentiment_df = tweet_df.iloc[:,1:]\n","    tweet_df['most_common_sentiment'] = sentiment_df.idxmax(axis=1)\n","\n","    sentiment_dict = {}\n","    ids, sentiments = tweet_df['id'].values.tolist(), tweet_df['most_common_sentiment'].values.tolist()\n","    for i in range(len(ids)):\n","        sentiment_dict[ids[i]] = sentiments[i]\n","\n","    if save_file:\n","        tweet_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/CS5344/sentiment.txt\", sep=',', index=False)\n","\n","    return tweet_df, sentiment_dict"],"metadata":{"id":"JiXWMJzv_5SD","executionInfo":{"status":"ok","timestamp":1667184181489,"user_tz":-480,"elapsed":6,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def postprocessing(id_df, sentiment_df, id_to_sentiment_dict):\n","    all_ids = pd.unique(id_df['id'])\n","    has_sentiment_label_ids = set(pd.unique(sentiment_df['id']))\n","\n","    # Add labels for those ids without valid sentences\n","    for id in all_ids:\n","        if id not in has_sentiment_label_ids:\n","            id_to_sentiment_dict[id] = 'neutral'\n","\n","    # Combine id_df and sentiment_df to get final result\n","    sentiment_predicted_results = pd.merge(id_df, sentiment_df, on=['id']).sort_values(['centerid', 'id'])\n","    \n","    return sentiment_predicted_results, id_to_sentiment_dict"],"metadata":{"id":"ke_JsNvbJUac","executionInfo":{"status":"ok","timestamp":1667184181490,"user_tz":-480,"elapsed":6,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Optional\n","from nltk.tokenize import sent_tokenize\n","\n","def sentences_tokenizer(inputs: pd.DataFrame):\n","    \"\"\"\n","    Auxiliary function to split text into sentences.\n","    inputs: A dataframe in the format of |user_id|tweet|\n","    outputs: A dataframe in the format of |user_id|sentences|\n","    Examples:\n","        I feel bad today. I think I should go to see a doctor. -> I feel bad today.\n","                                            I think I should go to see a doctor.\n","    \"\"\"\n","    outputs = inputs.copy(deep=True)\n","    outputs[\"sentences\"] = outputs[\"tweet\"].map(lambda tweet: sent_tokenize(tweet))\n","    outputs = outputs.drop([\"tweet\"], axis=1)\n","    outputs = outputs.explode(\"sentences\")\n","    outputs.columns = ['id', \"sentence\"]\n","    outputs = outputs.dropna()\n","\n","    outputs = outputs.reset_index()\n","    outputs = outputs.drop([\"index\"], axis=1)\n","    return outputs"],"metadata":{"id":"PUYvNrmhAL83","executionInfo":{"status":"ok","timestamp":1667184181490,"user_tz":-480,"elapsed":5,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### **Option A: Load the whole model**"],"metadata":{"id":"1247WWplxNP1"}},{"cell_type":"code","source":["# Load model\n","trained_model = load_model('/content/drive/MyDrive/Colab Notebooks/CS5344/lstm_model_v1')"],"metadata":{"id":"55h3AsZlecYy","executionInfo":{"status":"ok","timestamp":1667184188404,"user_tz":-480,"elapsed":6919,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Test\n","# tk_test = TweetTokenizer()\n","# tokens = preprocess_user_sentence(sentence, seqLen, eng_contractions, word_to_idx_dict, tk_test)\n","# tokens_in_numbers = get_tokens_in_numbers(tokens, seqLen)\n","# type_scores = trained_model.predict(tokens_in_numbers).tolist()\n","\n","# # 每个sample sentence会得到四个score，分别对应'happiness', 'fun', 'sadness', 'hate'\n","# 选择取最高的score，如果最高score大于某个阈值（例如0.7），就判断为该类，如果最高的score达不到阈值，判断为没有特殊感情的neutral类\n","# print(type_scores)"],"metadata":{"id":"8z9_Ybw6jJMJ","executionInfo":{"status":"ok","timestamp":1667184188404,"user_tz":-480,"elapsed":11,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### **Option B: Load model from a checkpoint**"],"metadata":{"id":"tGD9Soh_xfHi"}},{"cell_type":"code","source":["def create_lstm_model(seqLen, num_classes, vocab_size):\n","    input_tensor = Input(shape=(seqLen,), dtype='int32')\n","    mask = Masking(mask_value=0, input_shape=(seqLen, 50))(input_tensor)\n","    x = Embedding(vocab_size, 50, input_length=seqLen, trainable=False)(mask)\n","    x = LSTM(128, return_sequences=True)(x)\n","    x = LSTM(64, return_sequences=False)(x)\n","    x = Dense(16, activation='relu')(x)\n","    output_tensor = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(input_tensor, output_tensor)\n","    model.compile(optimizer=Adam(learning_rate=3e-4),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    model.summary()\n","    return model"],"metadata":{"id":"U5M6BdIJlm-w","executionInfo":{"status":"ok","timestamp":1667184188404,"user_tz":-480,"elapsed":10,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Load model from checkpoint dir\n","checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/CS5344/training_1'\n","latest_cp = tf.train.latest_checkpoint(checkpoint_dir)\n","# Create a new model instance\n","trained_model = create_lstm_model(seqLen, num_classes, vocab_size)\n","\n","# Load the previously saved weights\n","trained_model.load_weights(latest_cp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gjIHPSVlpmJ","outputId":"26257ae9-129a-4f04-eb83-66d4a8371916","executionInfo":{"status":"ok","timestamp":1667184190108,"user_tz":-480,"elapsed":1714,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 15)]              0         \n","                                                                 \n"," masking (Masking)           (None, 15)                0         \n","                                                                 \n"," embedding (Embedding)       (None, 15, 50)            972700    \n","                                                                 \n"," lstm (LSTM)                 (None, 15, 128)           91648     \n","                                                                 \n"," lstm_1 (LSTM)               (None, 64)                49408     \n","                                                                 \n"," dense (Dense)               (None, 16)                1040      \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 68        \n","                                                                 \n","=================================================================\n","Total params: 1,114,864\n","Trainable params: 142,164\n","Non-trainable params: 972,700\n","_________________________________________________________________\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2cdc08cc10>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Sample sentence\n","sentence = 'I feel bad today. I think I should go to see a doctor. @father'\n","# Test\n","tk_test = TweetTokenizer()\n","tokens = preprocess_user_sentence(sentence, seqLen, eng_contractions, word_to_idx_dict, tk_test)\n","# tokens_in_numbers = get_tokens_in_numbers(tokens, seqLen)\n","# type_scores = trained_model.predict(tokens_in_numbers).tolist()\n","type_scores = trained_model.predict(tokens).tolist()\n","\n","# 每个sample sentence会得到四个score，分别对应'happiness', 'fun', 'sadness', 'hate'\n","# 选择取最高的score，如果最高score大于某个阈值（例如0.7），就判断为该类，如果最高的score达不到阈值，判断为没有特殊感情的neutral类\n","print(type_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-He8q0GnluXI","outputId":"8f9d1e6a-1c86-4e37-f154-5815b27c87cf","executionInfo":{"status":"ok","timestamp":1667184191659,"user_tz":-480,"elapsed":1553,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 1s/step\n","[[0.024020923301577568, 0.002253579208627343, 0.9735829830169678, 0.0001425436494173482]]\n"]}]},{"cell_type":"markdown","source":["### **Make predictions based on user tweets**"],"metadata":{"id":"LUDoX7ebxy02"}},{"cell_type":"code","source":["input_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/CS5344/data/user_tweets.csv\", \n","                        encoding = 'unicode_escape',\n","                        index_col=0, na_filter=False)\n","input_df.drop_duplicates(subset=['id'], keep='first', inplace=True, ignore_index=True)\n","id_df = input_df[['id', 'centerid']]\n","inputs = input_df.drop(['centerid'], axis=1)\n","user_tweet_df = tweet_preprocess(inputs=inputs, minLen=minLen, tokenizer=tokenizer,sentence_tokenize=False)"],"metadata":{"id":"VeZC0owgyORt","executionInfo":{"status":"ok","timestamp":1667184193690,"user_tz":-480,"elapsed":2035,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["user_tweet_df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"VVwQdAqlN6oI","executionInfo":{"status":"ok","timestamp":1667184193690,"user_tz":-480,"elapsed":13,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}},"outputId":"97dcadad-df77-4f63-90a6-e5dbf9a90660"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         id tweet_index                                              tweet  \\\n","0  16190898    tweets_1       Visit to learn more about our special event!   \n","1  16190898    tweets_2  Unsurprisingly, there's a clearance sale on th...   \n","2  16190898    tweets_3  So lucky to have you as a sister. Happy birthd...   \n","3  16190898    tweets_4  Thank you NAB for the Distinguished Service Aw...   \n","4  16190898    tweets_5                        Look who stopped by today !   \n","5  16190898    tweets_6                                                      \n","6  16190898    tweets_7  Can I get a go Dawgs? Feeling much better and ...   \n","7  16190898    tweets_8  Everybody on and off set loved him. you'll be ...   \n","8  16190898    tweets_9                                                      \n","9  16190898   tweets_10  The biggest stars. The best music. Re-live #iH...   \n","\n","                                              tokens  \n","0  [809, 2, 807, 98, 65, 176, 692, 1492, 0, 0, 0,...  \n","1  [4, 4, 5, 4, 1267, 20, 368, 4, 0, 0, 0, 0, 0, ...  \n","2  [19, 512, 2, 15, 8, 89, 5, 626, 44, 261, 7106,...  \n","3  [156, 8, 19151, 13, 3, 4, 899, 2998, 113, 7, 2...  \n","4  [184, 182, 1107, 120, 46, 0, 0, 0, 0, 0, 0, 0,...  \n","5      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","6  [5, 42, 4, 162, 75, 124, 9, 164, 3, 358, 26, 6...  \n","7  [733, 20, 9, 91, 511, 374, 138, 4, 25, 6697, 2...  \n","8      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","9  [891, 7080, 113, 109, 4, 70, 7, 9, 396, 36, 3,...  "],"text/html":["\n","  <div id=\"df-b0a30a00-8998-46d4-af89-0cd76366ff3c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet_index</th>\n","      <th>tweet</th>\n","      <th>tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>16190898</td>\n","      <td>tweets_1</td>\n","      <td>Visit to learn more about our special event!</td>\n","      <td>[809, 2, 807, 98, 65, 176, 692, 1492, 0, 0, 0,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>16190898</td>\n","      <td>tweets_2</td>\n","      <td>Unsurprisingly, there's a clearance sale on th...</td>\n","      <td>[4, 4, 5, 4, 1267, 20, 368, 4, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16190898</td>\n","      <td>tweets_3</td>\n","      <td>So lucky to have you as a sister. Happy birthd...</td>\n","      <td>[19, 512, 2, 15, 8, 89, 5, 626, 44, 261, 7106,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16190898</td>\n","      <td>tweets_4</td>\n","      <td>Thank you NAB for the Distinguished Service Aw...</td>\n","      <td>[156, 8, 19151, 13, 3, 4, 899, 2998, 113, 7, 2...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16190898</td>\n","      <td>tweets_5</td>\n","      <td>Look who stopped by today !</td>\n","      <td>[184, 182, 1107, 120, 46, 0, 0, 0, 0, 0, 0, 0,...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>16190898</td>\n","      <td>tweets_6</td>\n","      <td></td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>16190898</td>\n","      <td>tweets_7</td>\n","      <td>Can I get a go Dawgs? Feeling much better and ...</td>\n","      <td>[5, 42, 4, 162, 75, 124, 9, 164, 3, 358, 26, 6...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>16190898</td>\n","      <td>tweets_8</td>\n","      <td>Everybody on and off set loved him. you'll be ...</td>\n","      <td>[733, 20, 9, 91, 511, 374, 138, 4, 25, 6697, 2...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>16190898</td>\n","      <td>tweets_9</td>\n","      <td></td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>16190898</td>\n","      <td>tweets_10</td>\n","      <td>The biggest stars. The best music. Re-live #iH...</td>\n","      <td>[891, 7080, 113, 109, 4, 70, 7, 9, 396, 36, 3,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0a30a00-8998-46d4-af89-0cd76366ff3c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b0a30a00-8998-46d4-af89-0cd76366ff3c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b0a30a00-8998-46d4-af89-0cd76366ff3c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["sentiment_df, partial_id_to_sentiment_dict = tweet_sentiment_predict(user_tweet_df, threshold=neutral_threshold, trained_model=trained_model, save_file=True)\n","sentiment_predicted_results, id_to_sentiment_dict = postprocessing(id_df, sentiment_df, partial_id_to_sentiment_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rM8I2Wsk74u8","executionInfo":{"status":"ok","timestamp":1667184197795,"user_tz":-480,"elapsed":4116,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}},"outputId":"87f758a8-657f-4a47-bbd6-d5982ccba92d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["265/265 [==============================] - 4s 13ms/step\n"]}]},{"cell_type":"code","source":["print(id_to_sentiment_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gt0BR-2fFhwL","executionInfo":{"status":"ok","timestamp":1667184197795,"user_tz":-480,"elapsed":8,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}},"outputId":"9f26df88-4b49-4e44-90c6-03b220910f23"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["{713143: 'neutral', 5625972: 'neutral', 5654712: 'neutral', 6753242: 'neutral', 7081402: 'neutral', 7215082: 'neutral', 7867072: 'neutral', 9721292: 'neutral', 11134252: 'neutral', 14124059: 'neutral', 14137737: 'neutral', 14262772: 'neutral', 14296157: 'neutral', 14342018: 'neutral', 14517899: 'neutral', 14708814: 'neutral', 14941525: 'neutral', 14982315: 'neutral', 14996887: 'neutral', 15074642: 'neutral', 15293352: 'neutral', 15428715: 'neutral', 15444539: 'neutral', 15566901: 'neutral', 15599390: 'neutral', 15635413: 'neutral', 15658327: 'neutral', 15741370: 'neutral', 16086928: 'neutral', 16149262: 'neutral', 16190898: 'neutral', 16212685: 'neutral', 16313405: 'neutral', 16331259: 'neutral', 16515888: 'neutral', 16548500: 'neutral', 16593362: 'neutral', 16745015: 'neutral', 17093604: 'neutral', 17487771: 'neutral', 17998609: 'neutral', 18088267: 'neutral', 18292600: 'neutral', 18857913: 'neutral', 18912121: 'neutral', 19074134: 'neutral', 19352747: 'neutral', 19409270: 'neutral', 19743731: 'neutral', 19772559: 'neutral', 19794082: 'neutral', 20113887: 'neutral', 20321006: 'neutral', 20455625: 'neutral', 21148185: 'neutral', 21308602: 'neutral', 21919642: 'neutral', 22745779: 'neutral', 22841103: 'neutral', 23000618: 'neutral', 23497233: 'neutral', 23642374: 'neutral', 23779324: 'neutral', 23873876: 'neutral', 24382752: 'neutral', 25034454: 'neutral', 25101704: 'neutral', 25768420: 'neutral', 26105653: 'neutral', 26140710: 'neutral', 26577824: 'neutral', 26642006: 'neutral', 26667184: 'neutral', 27294850: 'neutral', 27961547: 'neutral', 29514951: 'neutral', 29627447: 'neutral', 29818068: 'neutral', 31160337: 'neutral', 38151136: 'neutral', 39931366: 'neutral', 40519218: 'neutral', 41992925: 'neutral', 42208855: 'neutral', 43170448: 'neutral', 43803786: 'fun', 44967503: 'neutral', 45069293: 'neutral', 46468908: 'neutral', 46745299: 'neutral', 48799978: 'neutral', 55339804: 'neutral', 57795767: 'neutral', 58309829: 'neutral', 68900921: 'neutral', 70246837: 'neutral', 72348113: 'neutral', 72568426: 'neutral', 74231747: 'neutral', 75206471: 'neutral', 76587180: 'neutral', 76823111: 'neutral', 77383832: 'neutral', 80145924: 'neutral', 84239103: 'sadness', 84424885: 'neutral', 90777425: 'neutral', 94039455: 'neutral', 99596327: 'neutral', 99651997: 'neutral', 104221414: 'neutral', 113439399: 'neutral', 114870386: 'neutral', 117538430: 'neutral', 139183348: 'neutral', 169572927: 'neutral', 174144887: 'neutral', 174462575: 'neutral', 175954697: 'neutral', 177378345: 'neutral', 183763871: 'neutral', 188844665: 'neutral', 189283341: 'neutral', 191274945: 'neutral', 191692374: 'neutral', 194272236: 'neutral', 211643924: 'neutral', 219682445: 'neutral', 227436905: 'neutral', 234267541: 'neutral', 239514604: 'neutral', 250166431: 'neutral', 256113470: 'neutral', 266956478: 'happiness', 268607705: 'neutral', 277551680: 'neutral', 302288855: 'neutral', 322293052: 'neutral', 341251022: 'neutral', 346364970: 'neutral', 357211620: 'neutral', 362742545: 'neutral', 384047557: 'neutral', 387352949: 'neutral', 392508844: 'neutral', 394216985: 'neutral', 395192468: 'neutral', 423723318: 'neutral', 465833692: 'neutral', 494254095: 'neutral', 497420742: 'neutral', 524396430: 'neutral', 633287684: 'neutral', 718799630: 'neutral', 742051266: 'neutral', 820767955: 'neutral', 827115104: 'neutral', 828027691: 'neutral', 1249428006: 'neutral', 1374332138: 'neutral', 1561508208: 'neutral', 1705312428: 'neutral', 1884046142: 'neutral', 1967955732: 'neutral', 2250718737: 'neutral', 2845705185: 'neutral', 2896294831: 'neutral', 2927212800: 'neutral', 4369196414: 'neutral'}\n"]}]},{"cell_type":"code","source":["sentiment_predicted_results.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"N2CmCx2lHok5","executionInfo":{"status":"ok","timestamp":1667184197796,"user_tz":-480,"elapsed":7,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}},"outputId":"aa74d108-1846-4a8a-97e9-f9cf916021fc"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id  centerid   fun  happiness  hate  neutral  sadness  \\\n","43   5625972  16190898  0.12       0.10  0.00     0.68     0.10   \n","45   5654712  16190898  0.04       0.12  0.00     0.66     0.18   \n","28   6753242  16190898  0.14       0.08  0.04     0.66     0.08   \n","31   7215082  16190898  0.04       0.24  0.04     0.60     0.08   \n","37  15074642  16190898  0.12       0.06  0.00     0.76     0.06   \n","3   15566901  16190898  0.02       0.02  0.04     0.62     0.30   \n","33  15658327  16190898  0.02       0.10  0.06     0.70     0.12   \n","0   16190898  16190898  0.08       0.20  0.06     0.58     0.08   \n","6   16212685  16190898  0.06       0.08  0.10     0.66     0.10   \n","38  16331259  16190898  0.16       0.14  0.00     0.62     0.08   \n","\n","   most_common_sentiment  \n","43               neutral  \n","45               neutral  \n","28               neutral  \n","31               neutral  \n","37               neutral  \n","3                neutral  \n","33               neutral  \n","0                neutral  \n","6                neutral  \n","38               neutral  "],"text/html":["\n","  <div id=\"df-c75d9a4f-1f89-427f-88ca-fdd8ebaebe6f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>centerid</th>\n","      <th>fun</th>\n","      <th>happiness</th>\n","      <th>hate</th>\n","      <th>neutral</th>\n","      <th>sadness</th>\n","      <th>most_common_sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>43</th>\n","      <td>5625972</td>\n","      <td>16190898</td>\n","      <td>0.12</td>\n","      <td>0.10</td>\n","      <td>0.00</td>\n","      <td>0.68</td>\n","      <td>0.10</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>5654712</td>\n","      <td>16190898</td>\n","      <td>0.04</td>\n","      <td>0.12</td>\n","      <td>0.00</td>\n","      <td>0.66</td>\n","      <td>0.18</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>6753242</td>\n","      <td>16190898</td>\n","      <td>0.14</td>\n","      <td>0.08</td>\n","      <td>0.04</td>\n","      <td>0.66</td>\n","      <td>0.08</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>7215082</td>\n","      <td>16190898</td>\n","      <td>0.04</td>\n","      <td>0.24</td>\n","      <td>0.04</td>\n","      <td>0.60</td>\n","      <td>0.08</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>15074642</td>\n","      <td>16190898</td>\n","      <td>0.12</td>\n","      <td>0.06</td>\n","      <td>0.00</td>\n","      <td>0.76</td>\n","      <td>0.06</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15566901</td>\n","      <td>16190898</td>\n","      <td>0.02</td>\n","      <td>0.02</td>\n","      <td>0.04</td>\n","      <td>0.62</td>\n","      <td>0.30</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15658327</td>\n","      <td>16190898</td>\n","      <td>0.02</td>\n","      <td>0.10</td>\n","      <td>0.06</td>\n","      <td>0.70</td>\n","      <td>0.12</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>16190898</td>\n","      <td>16190898</td>\n","      <td>0.08</td>\n","      <td>0.20</td>\n","      <td>0.06</td>\n","      <td>0.58</td>\n","      <td>0.08</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>16212685</td>\n","      <td>16190898</td>\n","      <td>0.06</td>\n","      <td>0.08</td>\n","      <td>0.10</td>\n","      <td>0.66</td>\n","      <td>0.10</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>16331259</td>\n","      <td>16190898</td>\n","      <td>0.16</td>\n","      <td>0.14</td>\n","      <td>0.00</td>\n","      <td>0.62</td>\n","      <td>0.08</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c75d9a4f-1f89-427f-88ca-fdd8ebaebe6f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c75d9a4f-1f89-427f-88ca-fdd8ebaebe6f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c75d9a4f-1f89-427f-88ca-fdd8ebaebe6f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":[],"metadata":{"id":"9PL3yrvBFHib","executionInfo":{"status":"ok","timestamp":1667184197796,"user_tz":-480,"elapsed":6,"user":{"displayName":"He Wenbin","userId":"03124039579078048321"}}},"execution_count":21,"outputs":[]}]}