{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["1247WWplxNP1","tGD9Soh_xfHi"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pg4zIzDkaztl","outputId":"489d20c7-eaa0-4b86-e51f-6d686edb424d","executionInfo":{"status":"ok","timestamp":1667118767985,"user_tz":-480,"elapsed":2954,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import re\n","import string\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import json\n","import copy\n","from typing import Union\n","from nltk.tokenize import TweetTokenizer\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L92PR2IBcosb","outputId":"9f9aff9d-311a-4493-93ad-feeee237f7a4","executionInfo":{"status":"ok","timestamp":1667120210992,"user_tz":-480,"elapsed":7,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["import gensim\n","from gensim.models.keyedvectors import KeyedVectors"],"metadata":{"id":"90Clj-xwetEN","executionInfo":{"status":"ok","timestamp":1667120212442,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","print(tf.__version__)\n","\n","from tensorflow.keras import Sequential, Model, constraints\n","from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional, Masking, Layer\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7KXGxv2dCBY","outputId":"3d89627a-a248-468a-8985-52e964050f7f","executionInfo":{"status":"ok","timestamp":1667120213043,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.2\n"]}]},{"cell_type":"markdown","source":["### **Load necessary files**"],"metadata":{"id":"jZ-bFd_d_X1Z"}},{"cell_type":"code","source":["# Load necessary files\n","with open('/content/drive/MyDrive/Colab Notebooks/CS5344/english_contractions.json', 'r') as f1:\n","    eng_contractions = json.load(f1)\n","f1.close()\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/CS5344/vocab_to_idx.json', 'r') as f2:\n","    word_to_idx_dict = json.load(f2)"],"metadata":{"id":"_ZOJMbCWcnoK","executionInfo":{"status":"ok","timestamp":1667120221038,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["### **Define key parameters**"],"metadata":{"id":"azOcJzjWVpER"}},{"cell_type":"code","source":["seqLen = 15\n","minLen = 8\n","neutral_threshold = 0.8\n","tokenizer = TweetTokenizer()\n","\n","# Recording the relationship between idx and class\n","class_idx_dict = {'neutral':-1, 'happiness':0, 'fun':1, 'sadness':2, 'hate':3} # Since `neutral` label is not directly included in our prediction, we mark it as `-1`\n","idx_class_dict = {idx:class_label for class_label, idx in class_idx_dict.items()}"],"metadata":{"id":"CdC91vJsmFAj","executionInfo":{"status":"ok","timestamp":1667120222550,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Sample sentence\n","# sentence = 'I feel bad today. I think I should go to see a doctor. @father'"],"metadata":{"id":"oDaJzVn0gpIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Define necessary preprocessing functions**"],"metadata":{"id":"fWjrwWx0_UqA"}},{"cell_type":"code","source":["def normalize_contractions(sentence, eng_contractions_dict):\n","    return _normalize_contractions_text(sentence, eng_contractions_dict)\n","\n","def _normalize_contractions_text(text, contractions):\n","    \"\"\"\n","    This function normalizes english contractions (all input sentences in lower case).\n","    \"\"\"\n","    new_token_list = []\n","    token_list = text.split()\n","    for word_pos in range(len(token_list)):\n","        word = token_list[word_pos]\n","        if word in contractions:\n","            replacement = contractions[word]\n","\n","            first_rep = replacement.strip().split('/')[0]\n","            replacement_tokens = first_rep.strip().split()\n","            for w in replacement_tokens:\n","                new_token_list.append(w)\n","        else:\n","            new_token_list.append(word)\n","    sentence = \" \".join(new_token_list).strip(\" \")\n","    return sentence\n","\n","def simplify_punctuation_and_whitespace(sentence):\n","\n","    # print(\"Normalizing whitespaces and punctuation\")\n","    sent = _replace_urls(sentence)\n","    sent = _simplify_punctuation(sent)\n","    simplified_sent = _normalize_whitespace(sent)\n","      \n","    return simplified_sent\n","\n","def _replace_urls(text):\n","    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n","    text = re.sub(url_regex, \"<URL>\", text)\n","    return text\n","\n","def _simplify_punctuation(text):\n","    \"\"\"\n","    This function simplifies doubled or more complex punctuation. The exception is '...'.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n","    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n","    return corrected\n","\n","def _normalize_whitespace(text):\n","    \"\"\"\n","    This function normalizes whitespaces, removing duplicates.\n","    \"\"\"\n","    corrected = str(text)\n","    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n","    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n","    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n","    return corrected.strip(\" \")\n","\n","def reduce_exaggerations(text):\n","    \"\"\"\n","    Auxiliary function to help with exxagerated words.\n","    Examples:\n","        woooooords -> words\n","        yaaaaaaaaaaaaaaay -> yay\n","    \"\"\"\n","    correction = str(text)\n","    #TODO work on complexity reduction.\n","    return re.sub(r'(.)\\1+', r'\\1\\1', correction)\n","\n","def sentence_tokenizer(tk, sentence, word_to_idx_dict):\n","    words_list = tk.tokenize(sentence)\n","    tokenized_words_list = []\n","    i = 0\n","\n","    while i < len(words_list):\n","        if words_list[i].startswith('http') or words_list[i].startswith('www.'):\n","            i += 1\n","        elif words_list[i].endswith('.com'):\n","            i += 1\n","        # elif words_list[i-1] in string.punctuation and words_list[i] in string.punctuation:\n","        #     i += 1\n","        elif words_list[i] in string.punctuation:\n","            i += 1\n","        elif (len(words_list[i]) > 1) and (not (ord('a') <= ord(words_list[i][0]) <= ord('z'))) and (not words_list[i].startswith('<')):\n","            i += 1\n","        elif words_list[i].startswith('@'):\n","            tokenized_words_list.append('<person>')\n","            i += 1\n","        else:\n","            tokenized_words_list.append(words_list[i])\n","            i += 1\n","\n","    for j, w in enumerate(tokenized_words_list):\n","        if w not in word_to_idx_dict:\n","            tokenized_words_list[j] = 'unk'\n","\n","    return tokenized_words_list\n","\n","\n","# Main\n","def preprocess_user_sentence(sentence, eng_contractions,\n","                             word_to_idx_dict, \n","                             tokenizer):\n","    sentence = sentence.lower()\n","    s = normalize_contractions(sentence, eng_contractions)\n","    s = simplify_punctuation_and_whitespace(s)\n","    s = reduce_exaggerations(s)\n","    tokens = sentence_tokenizer(tokenizer, sentence, word_to_idx_dict)\n","    tokens_in_numbers = [word_to_idx_dict[w] for w in tokens]\n","\n","    return tokens_in_numbers"],"metadata":{"id":"RHdlpLZ9a7V1","executionInfo":{"status":"ok","timestamp":1667123993319,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":156,"outputs":[]},{"cell_type":"code","source":["def tweet_preprocess(inputs: Union[pd.DataFrame, pd.Series], minLen=8, tokenizer=TweetTokenizer(), sentence_tokenize=False):\n","    \"\"\"\n","    Function to preprocess tweets\n","    inputs: A dataframe in the format of |id|tweet1|tweet2|...|tweetn|, |id|tweet|, or |tweet|\n","    outputs: A dataframe in the format of |id|tweet_index|tweet|tokens|most_common_sentiment|\n","    sentence_tokenize: Determine if need to split text into sentences\n","    \"\"\"\n","\n","    data = inputs.copy(deep=True)\n","\n","    if data.shape[1]>2: # If there are various tweety in one row, transform them into the format of |id|tweet|\n","        data.columns = ['id'] + ['tweets_'+str(i) for i in range(1, data.shape[1])]\n","        data = data.set_index(['id'])\n","        data = data.stack()\n","        data = data.reset_index()\n","        data.columns = ['id', 'tweet_index', 'tweet']\n","        # data = data.drop(['tweet_index'], axis=1)\n","        # data.dropna()\n","\n","    elif data.shape[1]==2: # Data is in the format of |id|tweet|\n","        data.columns = ['id', 'tweet']\n","\n","    else: # Data is in the format of |tweet|\n","        data.columns = ['tweet']\n","    \n","    if sentence_tokenize: # Determine whether to split text into sentences\n","        data = sentences_tokenizer(data)\n","    \n","    # Standize some characters\n","    data['tweet'] = data['tweet'].map(lambda tweet: tweet.replace('¡¯', '\\'').replace('¡', '...'))\n","\n","    # Tokenize text\n","    data['tokens'] = data['tweet'].map(lambda tweet: preprocess_user_sentence(tweet, eng_contractions, word_to_idx_dict, tokenizer))\n","    data = data[data['tokens'].map(lambda x: len(x) > minLen)]\n","\n","    padded_sentence = pad_sequences(data['tokens'], maxlen=seqLen, padding='post', truncating='pre')\n","    data['tokens'] = [list(doc) for doc in padded_sentence]\n","\n","    return data\n","\n","\n","def tweet_sentiment_predict(tweet_df, threshold, trained_model, save_file=False):\n","    \"\"\"\n","    Function to predict tweet sentiment for different users\n","    threshold: When the highest score is lower than threshold, predicted label is set as `neutral`\n","    outputs: A dataframe in the format of |id|neutral|happiness|...|most_common_sentiment|; A dictionary containing ids and corresponding sentiments\n","    \"\"\"\n","    # Predict\n","    type_scores = trained_model.predict(np.asarray(tweet_df['tokens'].values.tolist()))\n","\n","    # Decode sentiment\n","    highest_scores = [np.max(score, axis=-1) for score in type_scores]\n","    idx_highest_scores = [np.argmax(score, axis=-1) for score in type_scores]\n","    labels = [idx_class_dict[idx] if score>=threshold else idx_class_dict[-1] for idx, score in zip(idx_highest_scores, highest_scores)]\n","    tweet_df[\"tag\"] = pd.DataFrame(labels)\n","\n","    # Drop useless column\n","    # if 'tweet_index' in tweet_df.columns:\n","    #     tweet_df = tweet_df.drop(['token'], axis=1)\n","    # if 'tweet' in tweet_df.columns:\n","    #     tweet_df = tweet_df.drop(['sentence'], axis=1)\n","\n","    # Count # of each type\n","    tweet_df = tweet_df.groupby(['id', 'tag'])['tag'].count()\n","    tweet_df = tweet_df.unstack()\n","    tweet_df = tweet_df.fillna(0)\n","    tweet_df = tweet_df.reset_index()\n","    tweet_df.iloc[:,1:] = tweet_df.iloc[:,1:].astype('int64')\n","    tweet_df.iloc[:,1:] = tweet_df.iloc[:,1:].div(tweet_df.iloc[:,1:].sum(axis=1), axis=0).round(4)\n","    \n","    # Find the most common type\n","    sentiment_df = tweet_df.iloc[:,1:]\n","    tweet_df['most_common_sentiment'] = sentiment_df.idxmax(axis=1)\n","\n","    sentiment_dict = {}\n","    ids, sentiments = tweet_df['id'].values.tolist(), tweet_df['most_common_sentiment'].values.tolist()\n","    for i in range(len(ids)):\n","        sentiment_dict[ids[i]] = sentiments[i]\n","\n","    if save_file:\n","        tweet_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/CS5344/sentiment.txt\", sep=',', index=False)\n","\n","    return tweet_df, sentiment_dict"],"metadata":{"id":"JiXWMJzv_5SD","executionInfo":{"status":"ok","timestamp":1667124014574,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":157,"outputs":[]},{"cell_type":"code","source":["def postprocessing(id_df, sentiment_df, id_to_sentiment_dict):\n","    all_ids = pd.unique(id_df['id'])\n","    has_sentiment_label_ids = set(pd.unique(sentiment_df['id']))\n","\n","    # Add labels for those ids without valid sentences\n","    for id in all_ids:\n","        if id not in has_sentiment_label_ids:\n","            id_to_sentiment_dict[id] = 'neutral'\n","\n","    # Combine id_df and sentiment_df to get final result\n","    sentiment_predicted_results = pd.merge(id_df, sentiment_df, on=['id']).sort_values(['centerid', 'id'])\n","    \n","    return sentiment_predicted_results, id_to_sentiment_dict"],"metadata":{"id":"ke_JsNvbJUac","executionInfo":{"status":"ok","timestamp":1667124017921,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":158,"outputs":[]},{"cell_type":"code","source":["# Optional\n","from nltk.tokenize import sent_tokenize\n","\n","def sentences_tokenizer(inputs: pd.DataFrame):\n","    \"\"\"\n","    Auxiliary function to split text into sentences.\n","    inputs: A dataframe in the format of |user_id|tweet|\n","    outputs: A dataframe in the format of |user_id|sentences|\n","    Examples:\n","        I feel bad today. I think I should go to see a doctor. -> I feel bad today.\n","                                            I think I should go to see a doctor.\n","    \"\"\"\n","    outputs = inputs.copy(deep=True)\n","    outputs[\"sentences\"] = outputs[\"tweet\"].map(lambda tweet: sent_tokenize(tweet))\n","    outputs = outputs.drop([\"tweet\"], axis=1)\n","    outputs = outputs.explode(\"sentences\")\n","    outputs.columns = ['id', \"sentence\"]\n","    outputs = outputs.dropna()\n","\n","    outputs = outputs.reset_index()\n","    outputs = outputs.drop([\"index\"], axis=1)\n","    return outputs"],"metadata":{"id":"PUYvNrmhAL83","executionInfo":{"status":"ok","timestamp":1667122121688,"user_tz":-480,"elapsed":7,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":["### **Option A: Load the whole model**"],"metadata":{"id":"1247WWplxNP1"}},{"cell_type":"code","source":["# Load model\n","trained_model = load_model('/content/drive/MyDrive/Colab Notebooks/CS5344/lstm_model_v1')"],"metadata":{"id":"55h3AsZlecYy","executionInfo":{"status":"ok","timestamp":1667122131522,"user_tz":-480,"elapsed":7140,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["# Test\n","# tk_test = TweetTokenizer()\n","# tokens = preprocess_user_sentence(sentence, eng_contractions, word_to_idx_dict, tk_test)\n","# tokens_in_numbers = get_tokens_in_numbers(tokens, seqLen)\n","# type_scores = trained_model.predict(tokens_in_numbers).tolist()\n","\n","# # 每个sample sentence会得到四个score，分别对应'happiness', 'fun', 'sadness', 'hate'\n","# 选择取最高的score，如果最高score大于某个阈值（例如0.7），就判断为该类，如果最高的score达不到阈值，判断为没有特殊感情的neutral类\n","# print(type_scores)"],"metadata":{"id":"8z9_Ybw6jJMJ","executionInfo":{"status":"ok","timestamp":1667122131523,"user_tz":-480,"elapsed":11,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":["### **Option B: Load model from a checkpoint**"],"metadata":{"id":"tGD9Soh_xfHi"}},{"cell_type":"code","source":["def create_lstm_model(seqLen, num_classes, vocab_size):\n","    input_tensor = Input(shape=(seqLen,), dtype='int32')\n","    mask = Masking(mask_value=0, input_shape=(seqLen, 50))(input_tensor)\n","    x = Embedding(vocab_size, 50, input_length=seqLen, trainable=False)(mask)\n","    x = LSTM(128, return_sequences=True)(x)\n","    x = LSTM(64, return_sequences=False)(x)\n","    x = Dense(16, activation='relu')(x)\n","    output_tensor = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(input_tensor, output_tensor)\n","    model.compile(optimizer=Adam(learning_rate=3e-4),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    model.summary()\n","    return model"],"metadata":{"id":"U5M6BdIJlm-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model from checkpoint dir\n","checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/CS5344/training_1'\n","latest_cp = tf.train.latest_checkpoint(checkpoint_dir)\n","# Create a new model instance\n","trained_model = create_lstm_model(seqLen, num_classes, vocab_size)\n","\n","# Load the previously saved weights\n","trained_model.load_weights(latest_cp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gjIHPSVlpmJ","outputId":"86ef1de4-68af-4d68-a10b-2b04253cfc33","executionInfo":{"status":"ok","timestamp":1667114643273,"user_tz":-480,"elapsed":1143,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 15)]              0         \n","                                                                 \n"," masking_1 (Masking)         (None, 15)                0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, 15, 50)            972700    \n","                                                                 \n"," lstm_2 (LSTM)               (None, 15, 128)           91648     \n","                                                                 \n"," lstm_3 (LSTM)               (None, 64)                49408     \n","                                                                 \n"," dense_2 (Dense)             (None, 16)                1040      \n","                                                                 \n"," dense_3 (Dense)             (None, 4)                 68        \n","                                                                 \n","=================================================================\n","Total params: 1,114,864\n","Trainable params: 142,164\n","Non-trainable params: 972,700\n","_________________________________________________________________\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa1adaead10>"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# Test\n","tk_test = TweetTokenizer()\n","tokens = preprocess_user_sentence(sentence, eng_contractions, word_to_idx_dict, tk_test)\n","tokens_in_numbers = get_tokens_in_numbers(tokens, seqLen)\n","type_scores = trained_model.predict(tokens_in_numbers).tolist()\n","\n","# 每个sample sentence会得到四个score，分别对应'happiness', 'fun', 'sadness', 'hate'\n","# 选择取最高的score，如果最高score大于某个阈值（例如0.7），就判断为该类，如果最高的score达不到阈值，判断为没有特殊感情的neutral类\n","print(type_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-He8q0GnluXI","outputId":"e63c1051-abe9-4152-c441-817c1ffe829b","executionInfo":{"status":"ok","timestamp":1667115970889,"user_tz":-480,"elapsed":26,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 19ms/step\n","[[0.024020923301577568, 0.002253579208627343, 0.9735829830169678, 0.0001425436494173482]]\n"]}]},{"cell_type":"markdown","source":["### **Make predictions based on user tweets**"],"metadata":{"id":"LUDoX7ebxy02"}},{"cell_type":"code","source":["input_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/CS5344/user_tweets.csv\", \n","                        encoding = 'unicode_escape',\n","                        index_col=0, na_filter=False)\n","input_df.drop_duplicates(subset=['id'], keep='first', inplace=True, ignore_index=True)\n","id_df = input_df[['id', 'centerid']]\n","inputs = input_df.drop(['centerid'], axis=1)\n","user_tweet_df = tweet_preprocess(inputs=inputs, minLen=minLen, tokenizer=tokenizer,sentence_tokenize=False)"],"metadata":{"id":"VeZC0owgyORt","executionInfo":{"status":"ok","timestamp":1667124021002,"user_tz":-480,"elapsed":1443,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}}},"execution_count":159,"outputs":[]},{"cell_type":"code","source":["user_tweet_df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"VVwQdAqlN6oI","executionInfo":{"status":"ok","timestamp":1667124031874,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}},"outputId":"03c0fc99-2dd9-43d8-f6aa-4ff3e0ba3c73"},"execution_count":161,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id tweet_index                                              tweet  \\\n","2   16190898    tweets_3  So lucky to have you as a sister. Happy birthd...   \n","3   16190898    tweets_4  Thank you NAB for the Distinguished Service Aw...   \n","6   16190898    tweets_7  Can I get a go Dawgs? Feeling much better and ...   \n","7   16190898    tweets_8  Everybody on and off set loved him. you'll be ...   \n","9   16190898   tweets_10  The biggest stars. The best music. Re-live #iH...   \n","10  16190898   tweets_11  Thank you for continuing to bring so much joy ...   \n","11  16190898   tweets_12  You won't believe what wrote... Her book \"Live...   \n","12  16190898   tweets_13  You can bet??on to find the best talent around...   \n","13  16190898   tweets_14  That's a wrap on #iHeartFestival2022 ! It's be...   \n","16  16190898   tweets_17  Night 2 is starting in just a few hours! #iHea...   \n","\n","                                               tokens  \n","2   [19, 512, 2, 15, 8, 89, 5, 626, 44, 261, 7106,...  \n","3   [156, 8, 19151, 13, 3, 4, 899, 2998, 113, 7, 2...  \n","6   [5, 42, 4, 162, 75, 124, 9, 164, 3, 358, 26, 6...  \n","7   [733, 20, 9, 91, 511, 374, 138, 4, 25, 6697, 2...  \n","9   [891, 7080, 113, 109, 4, 70, 7, 9, 396, 36, 3,...  \n","10  [156, 8, 13, 4, 2, 668, 19, 75, 1022, 2, 3, 39...  \n","11  [7803, 46, 9, 10, 2850, 26, 4, 2241, 17, 29, 4...  \n","12  [140, 1121, 288, 3870, 29, 656, 9, 4, 895, 203...  \n","13  [32, 3, 752, 182, 206, 7, 36, 9, 3, 824, 5564,...  \n","16  [68, 4, 10, 485, 12, 24, 5, 322, 207, 0, 0, 0,...  "],"text/html":["\n","  <div id=\"df-91204799-5233-48bf-8f4f-4eaf81098d6d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet_index</th>\n","      <th>tweet</th>\n","      <th>tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>16190898</td>\n","      <td>tweets_3</td>\n","      <td>So lucky to have you as a sister. Happy birthd...</td>\n","      <td>[19, 512, 2, 15, 8, 89, 5, 626, 44, 261, 7106,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16190898</td>\n","      <td>tweets_4</td>\n","      <td>Thank you NAB for the Distinguished Service Aw...</td>\n","      <td>[156, 8, 19151, 13, 3, 4, 899, 2998, 113, 7, 2...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>16190898</td>\n","      <td>tweets_7</td>\n","      <td>Can I get a go Dawgs? Feeling much better and ...</td>\n","      <td>[5, 42, 4, 162, 75, 124, 9, 164, 3, 358, 26, 6...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>16190898</td>\n","      <td>tweets_8</td>\n","      <td>Everybody on and off set loved him. you'll be ...</td>\n","      <td>[733, 20, 9, 91, 511, 374, 138, 4, 25, 6697, 2...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>16190898</td>\n","      <td>tweets_10</td>\n","      <td>The biggest stars. The best music. Re-live #iH...</td>\n","      <td>[891, 7080, 113, 109, 4, 70, 7, 9, 396, 36, 3,...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>16190898</td>\n","      <td>tweets_11</td>\n","      <td>Thank you for continuing to bring so much joy ...</td>\n","      <td>[156, 8, 13, 4, 2, 668, 19, 75, 1022, 2, 3, 39...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>16190898</td>\n","      <td>tweets_12</td>\n","      <td>You won't believe what wrote... Her book \"Live...</td>\n","      <td>[7803, 46, 9, 10, 2850, 26, 4, 2241, 17, 29, 4...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>16190898</td>\n","      <td>tweets_13</td>\n","      <td>You can bet??on to find the best talent around...</td>\n","      <td>[140, 1121, 288, 3870, 29, 656, 9, 4, 895, 203...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>16190898</td>\n","      <td>tweets_14</td>\n","      <td>That's a wrap on #iHeartFestival2022 ! It's be...</td>\n","      <td>[32, 3, 752, 182, 206, 7, 36, 9, 3, 824, 5564,...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16190898</td>\n","      <td>tweets_17</td>\n","      <td>Night 2 is starting in just a few hours! #iHea...</td>\n","      <td>[68, 4, 10, 485, 12, 24, 5, 322, 207, 0, 0, 0,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91204799-5233-48bf-8f4f-4eaf81098d6d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-91204799-5233-48bf-8f4f-4eaf81098d6d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-91204799-5233-48bf-8f4f-4eaf81098d6d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":161}]},{"cell_type":"code","source":["sentiment_df, partial_id_to_sentiment_dict = tweet_sentiment_predict(user_tweet_df, threshold=neutral_threshold, trained_model=trained_model, save_file=True)\n","sentiment_predicted_results, id_to_sentiment_dict = postprocessing(id_df, sentiment_df, partial_id_to_sentiment_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rM8I2Wsk74u8","executionInfo":{"status":"ok","timestamp":1667124051675,"user_tz":-480,"elapsed":4569,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}},"outputId":"d56dbc27-bbbf-49fa-f2ae-29e257447950"},"execution_count":162,"outputs":[{"output_type":"stream","name":"stdout","text":["121/121 [==============================] - 3s 15ms/step\n"]}]},{"cell_type":"code","source":["print(id_to_sentiment_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gt0BR-2fFhwL","executionInfo":{"status":"ok","timestamp":1667124052940,"user_tz":-480,"elapsed":7,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}},"outputId":"889710bb-a170-456c-bd03-fb3a0382d1dd"},"execution_count":163,"outputs":[{"output_type":"stream","name":"stdout","text":["{5625972: 'happiness', 5654712: 'neutral', 6753242: 'neutral', 7215082: 'fun', 7867072: 'fun', 9721292: 'neutral', 14262772: 'neutral', 14342018: 'neutral', 15074642: 'neutral', 15293352: 'sadness', 15444539: 'neutral', 15566901: 'neutral', 15658327: 'neutral', 16149262: 'fun', 16190898: 'neutral', 16212685: 'neutral', 16331259: 'happiness', 16515888: 'neutral', 16745015: 'sadness', 18912121: 'neutral', 19074134: 'neutral', 19409270: 'happiness', 19743731: 'fun', 19772559: 'neutral', 20455625: 'neutral', 21308602: 'happiness', 21919642: 'neutral', 22745779: 'happiness', 22841103: 'sadness', 23497233: 'neutral', 23642374: 'neutral', 23779324: 'sadness', 23873876: 'neutral', 24382752: 'neutral', 25768420: 'happiness', 26105653: 'sadness', 26140710: 'happiness', 26577824: 'fun', 26642006: 'fun', 27294850: 'happiness', 29627447: 'happiness', 38151136: 'sadness', 40519218: 'fun', 42208855: 'sadness', 43803786: 'sadness', 44967503: 'happiness', 46745299: 'neutral', 58309829: 'fun', 72348113: 'neutral', 72568426: 'neutral', 74231747: 'sadness', 76823111: 'neutral', 84239103: 'neutral', 84424885: 'sadness', 94039455: 'neutral', 113439399: 'happiness', 114870386: 'fun', 169572927: 'neutral', 174144887: 'happiness', 175954697: 'sadness', 177378345: 'fun', 189283341: 'neutral', 194272236: 'happiness', 219682445: 'sadness', 256113470: 'sadness', 384047557: 'neutral', 387352949: 'sadness', 394216985: 'neutral', 465833692: 'neutral', 494254095: 'neutral', 524396430: 'neutral', 718799630: 'neutral', 742051266: 'neutral', 1705312428: 'neutral', 2896294831: 'happiness', 2927212800: 'happiness', 4369196414: 'neutral', 18857913: 'neutral', 191274945: 'neutral', 68900921: 'neutral', 17093604: 'neutral', 14982315: 'neutral', 395192468: 'neutral', 346364970: 'neutral', 45069293: 'neutral', 11134252: 'neutral', 713143: 'neutral', 211643924: 'neutral', 25101704: 'neutral', 16086928: 'neutral', 14124059: 'neutral', 362742545: 'neutral', 14996887: 'neutral', 341251022: 'neutral', 14708814: 'neutral', 43170448: 'neutral', 29818068: 'neutral', 357211620: 'neutral', 75206471: 'neutral', 322293052: 'neutral', 19794082: 'neutral', 392508844: 'neutral', 14296157: 'neutral', 1967955732: 'neutral', 820767955: 'neutral', 2250718737: 'neutral', 31160337: 'neutral', 16593362: 'neutral', 1249428006: 'neutral', 183763871: 'neutral', 277551680: 'neutral', 1374332138: 'neutral', 99596327: 'neutral', 19352747: 'neutral', 17487771: 'neutral', 23000618: 'neutral', 827115104: 'neutral', 2845705185: 'neutral', 20321006: 'neutral', 250166431: 'neutral', 17998609: 'neutral', 70246837: 'neutral', 26667184: 'neutral', 14137737: 'neutral', 191692374: 'neutral', 29514951: 'neutral', 139183348: 'neutral', 76587180: 'neutral', 80145924: 'neutral', 14517899: 'neutral', 302288855: 'neutral', 20113887: 'neutral', 188844665: 'neutral', 7081402: 'neutral', 46468908: 'neutral', 423723318: 'neutral', 21148185: 'neutral', 90777425: 'neutral', 633287684: 'neutral', 497420742: 'neutral', 15428715: 'neutral', 39931366: 'neutral', 268607705: 'neutral', 48799978: 'neutral', 174462575: 'neutral', 1561508208: 'neutral', 99651997: 'neutral', 55339804: 'neutral', 18292600: 'neutral', 27961547: 'neutral', 16313405: 'neutral', 15635413: 'neutral', 234267541: 'neutral', 14941525: 'neutral', 266956478: 'neutral', 239514604: 'neutral', 15599390: 'neutral', 227436905: 'neutral', 15741370: 'neutral', 25034454: 'neutral', 57795767: 'neutral', 828027691: 'neutral', 117538430: 'neutral', 104221414: 'neutral', 1884046142: 'neutral', 18088267: 'neutral', 16548500: 'neutral', 41992925: 'neutral', 77383832: 'neutral'}\n"]}]},{"cell_type":"code","source":["sentiment_predicted_results.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"N2CmCx2lHok5","executionInfo":{"status":"ok","timestamp":1667124053594,"user_tz":-480,"elapsed":7,"user":{"displayName":"Zekai Zhou","userId":"00224331554234082278"}},"outputId":"8c0f23de-95eb-432f-ea17-bcc879bee109"},"execution_count":164,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id  centerid     fun  happiness    hate  neutral  sadness  \\\n","42   5625972  16190898  0.1818     0.2500  0.0909   0.2273   0.2500   \n","44   5654712  16190898  0.1064     0.2553  0.1064   0.3830   0.1489   \n","28   6753242  16190898  0.1429     0.2653  0.0204   0.3469   0.2245   \n","31   7215082  16190898  0.2500     0.2500  0.0000   0.2500   0.2500   \n","37  15074642  16190898  0.2000     0.1778  0.0889   0.3333   0.2000   \n","3   15566901  16190898  0.2000     0.2000  0.1111   0.2889   0.2000   \n","33  15658327  16190898  0.1429     0.2143  0.0714   0.2857   0.2857   \n","0   16190898  16190898  0.1034     0.1034  0.1034   0.4483   0.2414   \n","6   16212685  16190898  0.1667     0.2500  0.0000   0.3333   0.2500   \n","38  16331259  16190898  0.2128     0.2766  0.1064   0.1915   0.2128   \n","\n","   most_common_sentiment  \n","42             happiness  \n","44               neutral  \n","28               neutral  \n","31                   fun  \n","37               neutral  \n","3                neutral  \n","33               neutral  \n","0                neutral  \n","6                neutral  \n","38             happiness  "],"text/html":["\n","  <div id=\"df-50b590ba-0989-4a3f-a8bd-5bec15fec581\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>centerid</th>\n","      <th>fun</th>\n","      <th>happiness</th>\n","      <th>hate</th>\n","      <th>neutral</th>\n","      <th>sadness</th>\n","      <th>most_common_sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>42</th>\n","      <td>5625972</td>\n","      <td>16190898</td>\n","      <td>0.1818</td>\n","      <td>0.2500</td>\n","      <td>0.0909</td>\n","      <td>0.2273</td>\n","      <td>0.2500</td>\n","      <td>happiness</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>5654712</td>\n","      <td>16190898</td>\n","      <td>0.1064</td>\n","      <td>0.2553</td>\n","      <td>0.1064</td>\n","      <td>0.3830</td>\n","      <td>0.1489</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>6753242</td>\n","      <td>16190898</td>\n","      <td>0.1429</td>\n","      <td>0.2653</td>\n","      <td>0.0204</td>\n","      <td>0.3469</td>\n","      <td>0.2245</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>7215082</td>\n","      <td>16190898</td>\n","      <td>0.2500</td>\n","      <td>0.2500</td>\n","      <td>0.0000</td>\n","      <td>0.2500</td>\n","      <td>0.2500</td>\n","      <td>fun</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>15074642</td>\n","      <td>16190898</td>\n","      <td>0.2000</td>\n","      <td>0.1778</td>\n","      <td>0.0889</td>\n","      <td>0.3333</td>\n","      <td>0.2000</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15566901</td>\n","      <td>16190898</td>\n","      <td>0.2000</td>\n","      <td>0.2000</td>\n","      <td>0.1111</td>\n","      <td>0.2889</td>\n","      <td>0.2000</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15658327</td>\n","      <td>16190898</td>\n","      <td>0.1429</td>\n","      <td>0.2143</td>\n","      <td>0.0714</td>\n","      <td>0.2857</td>\n","      <td>0.2857</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>16190898</td>\n","      <td>16190898</td>\n","      <td>0.1034</td>\n","      <td>0.1034</td>\n","      <td>0.1034</td>\n","      <td>0.4483</td>\n","      <td>0.2414</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>16212685</td>\n","      <td>16190898</td>\n","      <td>0.1667</td>\n","      <td>0.2500</td>\n","      <td>0.0000</td>\n","      <td>0.3333</td>\n","      <td>0.2500</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>16331259</td>\n","      <td>16190898</td>\n","      <td>0.2128</td>\n","      <td>0.2766</td>\n","      <td>0.1064</td>\n","      <td>0.1915</td>\n","      <td>0.2128</td>\n","      <td>happiness</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50b590ba-0989-4a3f-a8bd-5bec15fec581')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-50b590ba-0989-4a3f-a8bd-5bec15fec581 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-50b590ba-0989-4a3f-a8bd-5bec15fec581');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":164}]},{"cell_type":"code","source":[],"metadata":{"id":"9PL3yrvBFHib"},"execution_count":null,"outputs":[]}]}